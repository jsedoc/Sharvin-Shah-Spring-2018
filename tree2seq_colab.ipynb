{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tree2seq_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/jsedoc/Sharvin-Shah-Spring-2018/blob/master/tree2seq_colab.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "4T2Bmn8UYSCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "f28bf56c-9039-40f6-be9f-975b1196618f"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version_info)\n",
        "\n",
        "!pip install http://download.pytorch.org/whl/cu90/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install tqdm nltk\n",
        "\n",
        "print(torch.cuda.device_count())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sys.version_info(major=3, minor=6, micro=3, releaselevel='final', serial=0)\n",
            "Requirement already satisfied: torch==0.3.0.post4 from http://download.pytorch.org/whl/cu90/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.3.0.post4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (1.14.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (3.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.23.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fdr6HphLgeU6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "85ec023e-eb1b-4d8e-facd-ef2d7184c3ab"
      },
      "cell_type": "code",
      "source": [
        "!rm -r data synth_t2t_data.zip\n",
        "!wget http://www.seas.upenn.edu/~joao/synth_t2t_data.zip\n",
        "!unzip synth_t2t_data.zip\n",
        "!pwd"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-05-01 20:06:39--  http://www.seas.upenn.edu/~joao/synth_t2t_data.zip\n",
            "Resolving www.seas.upenn.edu (www.seas.upenn.edu)... 158.130.68.91, 2607:f470:8:64:5ea5::9\n",
            "Connecting to www.seas.upenn.edu (www.seas.upenn.edu)|158.130.68.91|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10835 (11K) [application/zip]\n",
            "Saving to: ‘synth_t2t_data.zip’\n",
            "\n",
            "synth_t2t_data.zip  100%[===================>]  10.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-05-01 20:06:39 (114 MB/s) - ‘synth_t2t_data.zip’ saved [10835/10835]\n",
            "\n",
            "Archive:  synth_t2t_data.zip\n",
            "   creating: data/\n",
            "  inflating: data/train.orig         \n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iioumK_2fdKD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "\n",
        "PAD = 0\n",
        "UNK = 1\n",
        "BOS = 2\n",
        "EOS = 3\n",
        "\n",
        "PAD_WORD = '<blank>'\n",
        "UNK_WORD = '<unk>'\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "from nltk import Tree\n",
        "\n",
        "datapath = '/content/data/train.orig'\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def read_trees(self, filename):\n",
        "        with open(filename, 'r') as f:\n",
        "            trees = [Tree.fromstring(line) for line in tqdm(f.readlines())]\n",
        "        return trees\n",
        "    \n",
        "    def read_seqs(self, filename):\n",
        "        with open(filename, 'r') as f:\n",
        "            seqs = [line for line in tqdm(f.readlines())]\n",
        "        return seqs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TDLtCvQSfio4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e859d3d5-8267-4f96-8d7b-24f1b9de29c7"
      },
      "cell_type": "code",
      "source": [
        "dataset = Dataset()\n",
        "trees = dataset.read_trees('/content/data/train.orig')\n",
        "print(trees[1])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 33761.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(* (* (+ 55 (+ 56 53)) 31) (- 5 (* 54 9)))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "shVYtwDXgp6e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_vocab(filename):\n",
        "    vocab = dict()\n",
        "    with open(filename, 'r') as f:\n",
        "        for token in f.read().split():\n",
        "            if token not in vocab:\n",
        "                vocab[token] = 1\n",
        "            else:\n",
        "                vocab[token] += 1\n",
        "    print(len(vocab.keys()))    \n",
        "    index = 0\n",
        "    token_dict = {}            \n",
        "    for token in vocab.keys():\n",
        "        token_dict[token] = index\n",
        "        index += 1\n",
        "    \n",
        "    return token_dict\n",
        "\n",
        "def fetch_unique_index(token_dict, token):\n",
        "    return token_dict[token]\n",
        "\n",
        "def create_one_hot(token_dict):\n",
        "    one_hot_dict = {}\n",
        "    vector_dim = len(token_dict.keys())\n",
        "    for token in token_dict:    \n",
        "        tensor = torch.zeros(1, vector_dim)\n",
        "        tensor[0][token_dict[token]] = 1\n",
        "        one_hot_dict[token] = tensor\n",
        "    \n",
        "    return one_hot_dict\n",
        "\n",
        "\n",
        "def fetch_one_hot(one_hot_dict, token):\n",
        "    return one_hot_dict[token]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cGiPaybQk8uU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "a5f40c9d-b2b9-4ec1-e0ab-75692b7f5e9f"
      },
      "cell_type": "code",
      "source": [
        "vocab = create_vocab(datapath)\n",
        "print(vocab)\n",
        "oh_vocab = create_one_hot(vocab)\n",
        "print(fetch_one_hot(oh_vocab, '4'))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71\n",
            "{'(': 0, '+': 1, '43': 2, '47': 3, ')': 4, '*': 5, '55': 6, '56': 7, '53': 8, '31': 9, '-': 10, '5': 11, '54': 12, '9': 13, '/': 14, '4': 15, '1': 16, '39': 17, '50': 18, '23': 19, '52': 20, '51': 21, '30': 22, '57': 23, '42': 24, '11': 25, '6': 26, '20': 27, '59': 28, '0': 29, '58': 30, '19': 31, '29': 32, '38': 33, '28': 34, '27': 35, '16': 36, '44': 37, '60': 38, '34': 39, '36': 40, '32': 41, '35': 42, '8': 43, '33': 44, '7': 45, '12': 46, '3': 47, '15': 48, '21': 49, '13': 50, '17': 51, '18': 52, '61': 53, '49': 54, '10': 55, '40': 56, '2': 57, '46': 58, '48': 59, '14': 60, '37': 61, '45': 62, '22': 63, '26': 64, '41': 65, '24': 66, '25': 67, '62': 68, '63': 69, '64': 70}\n",
            "\n",
            "\n",
            "Columns 0 to 12 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "\n",
            "Columns 13 to 25 \n",
            "    0     0     1     0     0     0     0     0     0     0     0     0     0\n",
            "\n",
            "Columns 26 to 38 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "\n",
            "Columns 39 to 51 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "\n",
            "Columns 52 to 64 \n",
            "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "\n",
            "Columns 65 to 70 \n",
            "    0     0     0     0     0     0\n",
            "[torch.FloatTensor of size 1x71]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qwzbTnPDlKCv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5c2a41c3-1327-4466-c727-5800f47ba2c3"
      },
      "cell_type": "code",
      "source": [
        "dataset = Dataset()\n",
        "trees = dataset.read_trees(datapath)\n",
        "seqs = dataset.read_seqs(datapath)\n",
        "\n",
        "print(trees[1])\n",
        "print(seqs[1])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 29584.65it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1264868.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(* (* (+ 55 (+ 56 53)) 31) (- 5 (* 54 9)))\n",
            "( * ( * ( + 55 ( + 56 53 ) ) 31 ) ( - 5 ( * 54 9 ) ) )\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "E4j3b0VElUpx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable as Var\n",
        "\n",
        "PAD = 0\n",
        "UNK = 1\n",
        "BOS = 2\n",
        "EOS = 3\n",
        "\n",
        "PAD_WORD = '<blank>'\n",
        "UNK_WORD = '<unk>'\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "\n",
        "# module for childsumtreelstm\n",
        "class ChildSumTreeLSTM(nn.Module):\n",
        "    def __init__(self, in_dim, mem_dim):\n",
        "        super(ChildSumTreeLSTM, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.mem_dim = mem_dim\n",
        "        self.ioux = nn.Linear(self.in_dim, 3 * self.mem_dim)\n",
        "        self.iouh = nn.Linear(self.mem_dim, 3 * self.mem_dim)\n",
        "        self.fx = nn.Linear(self.in_dim, self.mem_dim)\n",
        "        self.fh = nn.Linear(self.mem_dim, self.mem_dim)\n",
        "\n",
        "    def node_forward(self, inputs, child_c, child_h):\n",
        "        child_h_sum = torch.sum(child_h, dim=0, keepdim=True)\n",
        "        print(\"dimension of inputs: \", inputs.size())\n",
        "        print(\"dimension of child_h_sum: \", child_h_sum.size())\n",
        "        iou = self.ioux(inputs) + self.iouh(child_h_sum)\n",
        "        i, o, u = torch.split(iou, iou.size(1) // 3, dim=1)\n",
        "        i, o, u = F.sigmoid(i), F.sigmoid(o), F.tanh(u)\n",
        "\n",
        "        f = F.sigmoid(\n",
        "            self.fh(child_h) +\n",
        "            self.fx(inputs).repeat(len(child_h), 1)\n",
        "        )\n",
        "        fc = torch.mul(f, child_c)\n",
        "\n",
        "        c = torch.mul(i, u) + torch.sum(fc, dim=0, keepdim=True)\n",
        "        h = torch.mul(o, F.tanh(c))\n",
        "        return c, h\n",
        "\n",
        "    def forward(self, tree):\n",
        "        print(type(tree))\n",
        "        print(tree)\n",
        "        if type(tree) == Tree:\n",
        "            child_states = []\n",
        "            for child in tree:\n",
        "                child_states.append(self.forward(child))\n",
        "            child_c, child_h = zip(* map(lambda x: x, child_states))\n",
        "            child_c, child_h = torch.cat(child_c, dim=0), torch.cat(child_h, dim=0)\n",
        "            \n",
        "            if type(tree) == str:\n",
        "                node_label_repr = Var(fetch_one_hot(oh_vocab, tree))\n",
        "            else:\n",
        "                node_label_repr = Var(fetch_one_hot(oh_vocab, tree.label()))\n",
        "            state = self.node_forward(node_label_repr, child_c, child_h)\n",
        "        else:\n",
        "            if type(tree) == str:\n",
        "                node_label_repr = Var(fetch_one_hot(oh_vocab, tree))\n",
        "            child_c = Var(node_label_repr.data.new(1, self.mem_dim).fill_(0.))\n",
        "            child_h = Var(node_label_repr.data.new(1, self.mem_dim).fill_(0.))\n",
        "            state = self.node_forward(node_label_repr, child_c, child_h)\n",
        "        # Using tree.label() instead of torch.zeros() ensures that the device placement is inherited  \n",
        "      \n",
        "        return state\n",
        "    \n",
        "\n",
        "\n",
        "# module for distance-angle similarity\n",
        "class Similarity(nn.Module):\n",
        "    def __init__(self, mem_dim, hidden_dim, num_classes):\n",
        "        super(Similarity, self).__init__()\n",
        "        self.mem_dim = mem_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.wh = nn.Linear(2 * self.mem_dim, self.hidden_dim)\n",
        "        self.wp = nn.Linear(self.hidden_dim, self.num_classes)\n",
        "\n",
        "    def forward(self, lvec, rvec):\n",
        "        mult_dist = torch.mul(lvec, rvec)\n",
        "        abs_dist = torch.abs(torch.add(lvec, -rvec))\n",
        "        vec_dist = torch.cat((mult_dist, abs_dist), 1)\n",
        "\n",
        "        out = F.sigmoid(self.wh(vec_dist))\n",
        "        out = F.log_softmax(self.wp(out), dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# putting the whole model together\n",
        "class SimilarityTreeLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, in_dim, mem_dim, hidden_dim, num_classes, sparsity, freeze):\n",
        "        super(SimilarityTreeLSTM, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, in_dim, padding_idx=PAD, sparse=sparsity)\n",
        "        if freeze:\n",
        "            self.emb.weight.requires_grad = False\n",
        "        self.childsumtreelstm = ChildSumTreeLSTM(in_dim, mem_dim)\n",
        "        self.similarity = Similarity(mem_dim, hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, ltree, linputs, rtree, rinputs):\n",
        "        linputs = self.emb(linputs)\n",
        "        rinputs = self.emb(rinputs)\n",
        "        lstate, lhidden = self.childsumtreelstm(ltree, linputs)\n",
        "        rstate, rhidden = self.childsumtreelstm(rtree, rinputs)\n",
        "        output = self.similarity(lstate, rstate)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NLd_-ggAlfWI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1003
        },
        "outputId": "c2842ccf-2dea-4a1b-bc84-e0dcc1c88866"
      },
      "cell_type": "code",
      "source": [
        "# encoder input size (in_dim) could be anything\n",
        "# encoder hidden size (mem_dim) would be equal to the dimension of the vector\n",
        "# encoder hidden size could be anything, decoder hidden size is dependent on final encoder hidden state size\n",
        "# decoder output size could be anything\n",
        "cs_treelstm = ChildSumTreeLSTM(71, 2)\n",
        "print(cs_treelstm)\n",
        "out = cs_treelstm.forward(trees[1])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ChildSumTreeLSTM(\n",
            "  (ioux): Linear(in_features=71, out_features=6)\n",
            "  (iouh): Linear(in_features=2, out_features=6)\n",
            "  (fx): Linear(in_features=71, out_features=2)\n",
            "  (fh): Linear(in_features=2, out_features=2)\n",
            ")\n",
            "<class 'nltk.tree.Tree'>\n",
            "(* (* (+ 55 (+ 56 53)) 31) (- 5 (* 54 9)))\n",
            "<class 'nltk.tree.Tree'>\n",
            "(* (+ 55 (+ 56 53)) 31)\n",
            "<class 'nltk.tree.Tree'>\n",
            "(+ 55 (+ 56 53))\n",
            "<class 'str'>\n",
            "55\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n",
            "<class 'nltk.tree.Tree'>\n",
            "(+ 56 53)\n",
            "<class 'str'>\n",
            "56\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n",
            "<class 'str'>\n",
            "53\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n",
            "<class 'str'>\n",
            "31\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n",
            "<class 'nltk.tree.Tree'>\n",
            "(- 5 (* 54 9))\n",
            "<class 'str'>\n",
            "5\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n",
            "<class 'nltk.tree.Tree'>\n",
            "(* 54 9)\n",
            "<class 'str'>\n",
            "54\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n",
            "<class 'str'>\n",
            "9\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n",
            "dimension of inputs:  torch.Size([1, 71])\n",
            "dimension of child_h_sum:  torch.Size([1, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nTpUCcssloT5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q0aX1zKGlsk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        print(\"input size: \", input.size())\n",
        "        print(\"hidden size: \", hidden.size())\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        print(\"After Embedding, size: \", output.size())\n",
        "        output = F.relu(output)\n",
        "        print(\"output size after ReLU: \", output.size())\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
        "        if use_cuda:\n",
        "            return result.cuda()\n",
        "        else:\n",
        "            return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Qi5m6ZilyI3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def token_to_index(token_dict, filename):\n",
        "    file_embedding = []\n",
        "    \n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f.readlines()[:2]:\n",
        "            line_embedding = []\n",
        "            print(\"line: \", line)\n",
        "            tokens = line.split()\n",
        "            print(\"tokens: \", tokens)\n",
        "            for token in tokens:\n",
        "                line_embedding.append(token_dict[token])\n",
        "            \n",
        "            print(\"line embedding: \", line_embedding)\n",
        "            line_tensor = Var(torch.LongTensor(line_embedding).view(-1, 1))\n",
        "            print(\"line tensor: \", line_tensor)\n",
        "            file_embedding.append(line_tensor)\n",
        "            print(\"file embedding so far: \", file_embedding)\n",
        "    \n",
        "    return file_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fsAmL7GKl3q4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1975
        },
        "outputId": "3995c1d6-8f38-4e38-daa0-17a3d841f16a"
      },
      "cell_type": "code",
      "source": [
        "emb = token_to_index(vocab, datapath)\n",
        "emb[1]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "line:  ( + 43 47 )\n",
            "\n",
            "tokens:  ['(', '+', '43', '47', ')']\n",
            "line embedding:  [0, 1, 2, 3, 4]\n",
            "line tensor:  Variable containing:\n",
            " 0\n",
            " 1\n",
            " 2\n",
            " 3\n",
            " 4\n",
            "[torch.LongTensor of size 5x1]\n",
            "\n",
            "file embedding so far:  [Variable containing:\n",
            " 0\n",
            " 1\n",
            " 2\n",
            " 3\n",
            " 4\n",
            "[torch.LongTensor of size 5x1]\n",
            "]\n",
            "line:  ( * ( * ( + 55 ( + 56 53 ) ) 31 ) ( - 5 ( * 54 9 ) ) )\n",
            "\n",
            "tokens:  ['(', '*', '(', '*', '(', '+', '55', '(', '+', '56', '53', ')', ')', '31', ')', '(', '-', '5', '(', '*', '54', '9', ')', ')', ')']\n",
            "line embedding:  [0, 5, 0, 5, 0, 1, 6, 0, 1, 7, 8, 4, 4, 9, 4, 0, 10, 11, 0, 5, 12, 13, 4, 4, 4]\n",
            "line tensor:  Variable containing:\n",
            "    0\n",
            "    5\n",
            "    0\n",
            "    5\n",
            "    0\n",
            "    1\n",
            "    6\n",
            "    0\n",
            "    1\n",
            "    7\n",
            "    8\n",
            "    4\n",
            "    4\n",
            "    9\n",
            "    4\n",
            "    0\n",
            "   10\n",
            "   11\n",
            "    0\n",
            "    5\n",
            "   12\n",
            "   13\n",
            "    4\n",
            "    4\n",
            "    4\n",
            "[torch.LongTensor of size 25x1]\n",
            "\n",
            "file embedding so far:  [Variable containing:\n",
            " 0\n",
            " 1\n",
            " 2\n",
            " 3\n",
            " 4\n",
            "[torch.LongTensor of size 5x1]\n",
            ", Variable containing:\n",
            "    0\n",
            "    5\n",
            "    0\n",
            "    5\n",
            "    0\n",
            "    1\n",
            "    6\n",
            "    0\n",
            "    1\n",
            "    7\n",
            "    8\n",
            "    4\n",
            "    4\n",
            "    9\n",
            "    4\n",
            "    0\n",
            "   10\n",
            "   11\n",
            "    0\n",
            "    5\n",
            "   12\n",
            "   13\n",
            "    4\n",
            "    4\n",
            "    4\n",
            "[torch.LongTensor of size 25x1]\n",
            "]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable containing:\n",
              "    0\n",
              "    5\n",
              "    0\n",
              "    5\n",
              "    0\n",
              "    1\n",
              "    6\n",
              "    0\n",
              "    1\n",
              "    7\n",
              "    8\n",
              "    4\n",
              "    4\n",
              "    9\n",
              "    4\n",
              "    0\n",
              "   10\n",
              "   11\n",
              "    0\n",
              "    5\n",
              "   12\n",
              "   13\n",
              "    4\n",
              "    4\n",
              "    4\n",
              "[torch.LongTensor of size 25x1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "RmBw8SqnmI0z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5e739a2-4d89-47fa-dd16-c27882960b5e"
      },
      "cell_type": "code",
      "source": [
        "out[1].size()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "MBwzwPXymS8e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0efc43d-b641-489a-b2e4-6295e425e099"
      },
      "cell_type": "code",
      "source": [
        "len(vocab.keys())"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "jDXFjKSfmXs_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "c9ce0d9e-5823-4d3e-c0dd-fcd5fe62414f"
      },
      "cell_type": "code",
      "source": [
        "decoder = DecoderRNN(out[1].size()[1], len(vocab.keys()))\n",
        "#hidden = decoder.initHidden()\n",
        "encoder_outputs = out[1]\n",
        "encoder_outputs = encoder_outputs.unsqueeze(0)\n",
        "print(encoder_outputs)\n",
        "for i in range(out[1].size()[1]):\n",
        "  out, hidden = decoder.forward(emb[1][i], encoder_outputs)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            "(0 ,.,.) = \n",
            "  0.2348 -0.3339\n",
            "[torch.FloatTensor of size 1x1x2]\n",
            "\n",
            "input size:  torch.Size([1])\n",
            "hidden size:  torch.Size([1, 1, 2])\n",
            "After Embedding, size:  torch.Size([1, 1, 2])\n",
            "output size after ReLU:  torch.Size([1, 1, 2])\n",
            "input size:  torch.Size([1])\n",
            "hidden size:  torch.Size([1, 1, 2])\n",
            "After Embedding, size:  torch.Size([1, 1, 2])\n",
            "output size after ReLU:  torch.Size([1, 1, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "aYb0LtW78E9J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "12767e83-8bde-49ed-bfbb-82598c52c32f"
      },
      "cell_type": "code",
      "source": [
        "out"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable containing:\n",
              "\n",
              "Columns 0 to 9 \n",
              "-5.0534 -4.6585 -4.1347 -4.4704 -3.9207 -5.1462 -4.0097 -3.6419 -4.4979 -3.7046\n",
              "\n",
              "Columns 10 to 19 \n",
              "-4.6966 -4.4008 -4.3315 -4.0058 -4.5834 -4.0512 -3.6190 -4.0552 -4.6586 -4.7106\n",
              "\n",
              "Columns 20 to 29 \n",
              "-3.9462 -3.5929 -3.8332 -4.6592 -4.7064 -3.9936 -4.1536 -3.9050 -4.1274 -4.2782\n",
              "\n",
              "Columns 30 to 39 \n",
              "-4.4027 -4.7186 -3.9313 -4.6256 -3.3866 -4.5186 -4.9716 -4.3361 -5.2079 -3.8708\n",
              "\n",
              "Columns 40 to 49 \n",
              "-4.4456 -3.3501 -4.5868 -4.8579 -4.4372 -4.1422 -4.6728 -5.0819 -4.3704 -4.8397\n",
              "\n",
              "Columns 50 to 59 \n",
              "-4.1965 -4.6392 -4.1611 -4.1451 -4.4054 -4.7596 -3.9315 -3.8555 -4.7831 -3.7334\n",
              "\n",
              "Columns 60 to 69 \n",
              "-5.0541 -5.1496 -4.1554 -4.5686 -4.8764 -4.4532 -5.1694 -4.5786 -4.2481 -4.8181\n",
              "\n",
              "Columns 70 to 70 \n",
              "-3.8813\n",
              "[torch.FloatTensor of size 1x71]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "-qWCQti8-zxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "019252b9-6f53-4106-e3ef-00bfed26c763"
      },
      "cell_type": "code",
      "source": [
        "def train(tree_variable, target_variable, \n",
        "          encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
        "          criterion, max_length=MAX_LENGTH):\n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "  \n",
        "  # encode tree\n",
        "  \n",
        "  # decoder serial and accumulate loss\n",
        "  \n",
        "  # call backward\n",
        "  \n",
        "  # update parameters"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-204ab9f81e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m def train(tree_variable, target_variable, \n\u001b[1;32m      2\u001b[0m           \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           criterion, max_length=MAX_LENGTH):\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MAX_LENGTH' is not defined"
          ]
        }
      ]
    }
  ]
}