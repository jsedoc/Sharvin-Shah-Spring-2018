chapters 6,7,8,10 http://www.deeplearningbook.org/
The original sequence to sequence paper https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf

One of the original attention paper https://arxiv.org/pdf/1409.0473.pdf

For the class last spring when we covered attention, I used https://talbaumel.github.io/attention/ for figures and simplified explanation.

I also pointed everyone to Chris Olah's blog https://distill.pub/2016/augmented-rnns/

Although to mention how they work, I think I gained the most insight into this from the pointer networks paper https://arxiv.org/pdf/1506.03134.pdf. For some reason, I had really thought of attention closer to the pointer network. 
